# Transformers and Embeddings

## Objetivos
- Aprender a arquitetura do Transformer e a autoatenção.
- Explorar os embeddings de palavras e frases (Word2Vec, GloVe, BERT).
- Estudar os mecanismos de atenção e a codificação posicional.

## Leituras
- "Attention is All You Need" (Atenção é tudo o que você precisa) - Vaswani et al. (2017).
- “BERT: pré-treinamento de transformadores bidirecionais profundos” - Devlin et al. (2018).
- The Illustrated Transformer* (O transformador ilustrado) - Jay Alammar.

## Practice
- Use Hugging Face Transformers para executar o BERT ou o DistilBERT.
- Treine um pequeno modelo de transformador usando `transformers` + `datasets`.
- Explore a similaridade semântica usando embeddings de sentença.

## Notas
- Capture como os transformadores diferem dos RNNs em termos de arquitetura e desempenho.
- Observe os paradigmas de pré-treinamento e ajuste fino.
