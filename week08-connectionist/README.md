# Transformers and Embeddings

## Objectives
- Learn the Transformer architecture and self-attention.
- Explore word and sentence embeddings (Word2Vec, GloVe, BERT).
- Study attention mechanisms and positional encoding.

## Readings
- "Attention is All You Need" – Vaswani et al. (2017).
- "BERT: Pre-training of Deep Bidirectional Transformers" – Devlin et al. (2018).
- *The Illustrated Transformer* – Jay Alammar.

## Practice
- Use Hugging Face Transformers to run BERT or DistilBERT.
- Train a small transformer model using `transformers` + `datasets`.
- Explore semantic similarity using sentence embeddings.

## Notes
- Capture how transformers differ from RNNs in architecture and performance.
- Note pre-training and fine-tuning paradigms.
