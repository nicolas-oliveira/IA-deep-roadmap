# Deep Networks

## Objectives
- Understand deep feedforward networks and their optimization.
- Learn about vanishing gradients, initialization, and batch normalization.
- Explore optimizers like Adam, RMSProp, and SGD.

## Readings
- *Deep Learning* – Ian Goodfellow, Chapters 8–10.
- "A Brief Survey on Deep Learning" – LeCun, Bengio, Hinton (Nature 2015).
- "Understanding the difficulty of training deep feedforward networks" – Glorot & Bengio (2010).

## Practice
- Train a deep neural network using PyTorch or TensorFlow.
- Explore how layer depth affects performance on MNIST or CIFAR-10.
- Experiment with optimizers and learning rate schedules.

## Notes
- Note the challenges and techniques for stabilizing deep training.
- Summarize the role of normalization and initialization strategies.
